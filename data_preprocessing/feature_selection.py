from statistics import mean

from scipy.stats import entropy
from sklearn.feature_selection import mutual_info_classif


def mutual_info_selection(dataframe, targets):
    dataframe['text'] = dataframe['text'].astype('category')
    dataframe['header'] = dataframe['header'].astype('category')
    fs = (mutual_info_classif(dataframe, targets, discrete_features=True))
    for i in range(0, len(dataframe.columns)):
        col_entropy = calc_entropy(dataframe[dataframe.columns[i]])
        entropy_vec = [min(calc_entropy(dataframe[col]), col_entropy) for col in
                       dataframe.drop(dataframe.columns[i], axis=1)]
        values = mutual_info_classif(dataframe.drop(dataframe.columns[i], axis=1), dataframe[dataframe.columns[i]],
                                     discrete_features=True) / entropy_vec
        fs[i] = fs[i] - mean(values)
    cols_to_drop = {key: value for (key, value) in zip(dataframe.columns, fs)}
    cols_to_drop = {k: v for k, v in sorted(cols_to_drop.items(), key=lambda item: item[1])}
    return list(cols_to_drop.keys())


def calc_entropy(column):
    values = column.value_counts()
    values = values / len(column)
    return entropy(values, base=2)
